# Опис:
Цей проєкт реалізує етап «Видобування даних» у процесі обробки великого набору погодних даних з Kaggle (Indian 5000 Cities Weather Data).
Дані зчитуються за допомогою Apache Spark у контейнері Docker.
На виході отримується схема даних, загальна інформація про набір (кількість рядків і колонок), а також приклади перших рядків таблиці.
Подальші етапи проєкту — трансформація, аналітика та побудова бізнес-запитів.

# Структура проєкту

BigData/
│── data/ — локальна тека з CSV-файлами
│── src/ — пакет із модулем io_utils.py, який містить схему та функцію зчитування
│── main.py — головний файл запуску
│── Dockerfile — інструкція для створення контейнера
│── .dockerignore — файли, які не копіюються всередину образу
│── README.md — опис проєкту

# Вимоги перед запуском

• Встановлений Python 3.9 або новіший
• Встановлений Docker Desktop
• Встановлений Git

# Кроки розгортання

Клонувати репозиторій з GitHub:
git clone https://github.com/
<team>/<BigDataProject>.git
cd BigData

Завантажити датасет з Kaggle:
https://www.kaggle.com/datasets/mukeshdevrath007/indian-5000-cities-weather-data

Лише частину файлів (теку w_d_1) у локальну папку data у корені проєкту.
Для macOS – /Users/<name>/PycharmProjects/BigData/data/
Для Windows – C:\Users<name>\BigData\data\

Папку data у Git не додаємо, бо вона займає багато місця.

Перевірити, що Docker працює:
docker --version
docker run hello-world

Створити контейнер:
docker build -t my-spark-img .

Запустити програму, підключивши локальні дані:
• для macOS/Linux:
docker run -v /Users/<name>/PycharmProjects/BigData/data:/app/data my-spark-img
• для Windows:
docker run -v C:\Users<name>\BigData\data:/app/data my-spark-img

# Результат виконання

Програма створює Spark-сесію, читає всі CSV-файли з погодними даними,
виводить кількість рядків і колонок, перелік назв стовпців, схему DataFrame
та перші 5 рядків таблиці у зручному табличному форматі.

# Наприклад:

=== Етап видобування даних ===
Кількість рядків: 218 375 232
Кількість колонок: 21
...
Схема DataFrame:
|-- anonymous: string (nullable = true)
|-- date: timestamp (nullable = true)
|-- temperature_2m: double (nullable = true)
...
Перші 5 рядків даних – таблиця з усіма полями.

# Що реалізовано

• Створено повну схему даних із 21 стовпця (модуль io_utils.py).
• Реалізовано функцію load_weather_data для зчитування CSV-файлів згідно зі схемою.
• Реалізовано головний скрипт main.py для запуску процесу.
• Вивід структурований і готовий до подальшої трансформації.

# Рекомендації для командної роботи
• Перед пушем протестувати локально:
docker build -t my-spark-img .
docker run -v /path/to/data:/app/data my-spark-img

# Результат етапу

Пункт «Створіть відповідні схеми для набору даних» виконано.
Система готова до наступних пунктів частини.